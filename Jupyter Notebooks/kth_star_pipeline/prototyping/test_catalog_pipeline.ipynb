{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a117c01-7db2-4851-be95-2222e9d991e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries.\n"
     ]
    }
   ],
   "source": [
    "# Dask puts out more advisory logging that we care for.\n",
    "# It takes some doing to quiet all of it, but this recipe works.\n",
    "import dask\n",
    "import logging\n",
    "import dask_jobqueue\n",
    "from dask.dataframe.utils import make_meta\n",
    "from dask.distributed import Client\n",
    "\n",
    "dask.config.set({\"logging.distributed\": \"critical\"})\n",
    "\n",
    "# This also has to be done, for the above to be effective\n",
    "logger = logging.getLogger(\"distributed\")\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "# Finally, suppress the specific warning about Dask dashboard port usage\n",
    "warnings.filterwarnings(\"ignore\", message=\"Port 8787 is already in use.\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from hats import read_hats\n",
    "\n",
    "import lsdb\n",
    "\n",
    "from catalog_filtering import bandFilterLenient, contains_PM\n",
    "import hpms_pipeline as hpms\n",
    "\n",
    "print(\"Imported libraries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6140124c-83c2-4335-a0c8-2b269bf77fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined vars\n"
     ]
    }
   ],
   "source": [
    "# Directory variables\n",
    "BENCHMARK_DIR = Path(\"../../../../catalogs/benchmark_catalogs\")\n",
    "two_deg_cs_name = \"2.306965202564744e+18\"\n",
    "TWO_DEG_CS_DIR = BENCHMARK_DIR / two_deg_cs_name\n",
    "mc_name = two_deg_cs_name.replace(\".\", \"\", 1).replace(\"+\", \"\", 1)  + \"_25_arcsec_mc\"\n",
    "MC_DIR = BENCHMARK_DIR / mc_name\n",
    "two_deg_cs_catalog = lsdb.read_hats(TWO_DEG_CS_DIR, margin_cache=MC_DIR)\n",
    "\n",
    "# Filter variables\n",
    "bandList = ['G','R','I','Z','Y']\n",
    "class_star = None\n",
    "spread_model = 0.05\n",
    "magnitude_error = 0.05\n",
    "check_flags = True\n",
    "mag_cutoff = 19\n",
    "query_string = bandFilterLenient(\n",
    "    bandList,\n",
    "    classStar=class_star,\n",
    "    spreadModel=spread_model,\n",
    "    magError=magnitude_error,\n",
    "    flag=check_flags,\n",
    "    mag=19\n",
    ")\n",
    "des_cols = (\n",
    "    [f'CLASS_STAR_{band}' for band in bandList] + \n",
    "    [f'FLAGS_{band}' for band in bandList] + \n",
    "    ['RA','DEC','COADD_OBJECT_ID'] + \n",
    "    [f'SPREAD_MODEL_{band}' for band in bandList] + \n",
    "    [f'WAVG_MAG_PSF_{band}' for band in bandList] + \n",
    "    [f'WAVG_MAGERR_PSF_{band}' for band in bandList]\n",
    ")\n",
    "des_id_col = 'COADD_OBJECT_ID_1'\n",
    "mag_cols = [f'WAVG_MAG_PSF_{band}' for band in ['G','I']]\n",
    "\n",
    "#Algorithm variables\n",
    "k = 2\n",
    "max_obj_deviation = 0.2\n",
    "cone_search_rad = 30\n",
    "max_neighbor_dist = 25\n",
    "xmatch_max_neighbors = 100\n",
    "min_neighbors = 3\n",
    "print(\"Defined vars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d15845-bace-42bf-aa44-1cacb4079451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>lsdb Catalog 2.306965202564744e+18:</strong></div><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS_STAR_G</th>\n",
       "      <th>CLASS_STAR_R</th>\n",
       "      <th>CLASS_STAR_I</th>\n",
       "      <th>CLASS_STAR_Z</th>\n",
       "      <th>CLASS_STAR_Y</th>\n",
       "      <th>FLAGS_G</th>\n",
       "      <th>FLAGS_R</th>\n",
       "      <th>FLAGS_I</th>\n",
       "      <th>FLAGS_Z</th>\n",
       "      <th>FLAGS_Y</th>\n",
       "      <th>RA</th>\n",
       "      <th>DEC</th>\n",
       "      <th>COADD_OBJECT_ID</th>\n",
       "      <th>SPREAD_MODEL_G</th>\n",
       "      <th>SPREAD_MODEL_R</th>\n",
       "      <th>SPREAD_MODEL_I</th>\n",
       "      <th>SPREAD_MODEL_Z</th>\n",
       "      <th>SPREAD_MODEL_Y</th>\n",
       "      <th>WAVG_MAG_PSF_G</th>\n",
       "      <th>WAVG_MAG_PSF_R</th>\n",
       "      <th>WAVG_MAG_PSF_I</th>\n",
       "      <th>WAVG_MAG_PSF_Z</th>\n",
       "      <th>WAVG_MAG_PSF_Y</th>\n",
       "      <th>WAVG_MAGERR_PSF_G</th>\n",
       "      <th>WAVG_MAGERR_PSF_R</th>\n",
       "      <th>WAVG_MAGERR_PSF_I</th>\n",
       "      <th>WAVG_MAGERR_PSF_Z</th>\n",
       "      <th>WAVG_MAGERR_PSF_Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Order: 5, Pixel: 4096</th>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>int16[pyarrow]</td>\n",
       "      <td>int16[pyarrow]</td>\n",
       "      <td>int16[pyarrow]</td>\n",
       "      <td>int16[pyarrow]</td>\n",
       "      <td>int16[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>int64[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "      <td>double[pyarrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order: 5, Pixel: 4097</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order: 5, Pixel: 8875</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order: 5, Pixel: 8878</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><div>The catalog has been loaded <strong>lazily</strong>, meaning no data has been read, only the catalog schema</div>"
      ],
      "text/plain": [
       "Dask NestedFrame Structure:\n",
       "                        CLASS_STAR_G     CLASS_STAR_R     CLASS_STAR_I     CLASS_STAR_Z     CLASS_STAR_Y         FLAGS_G         FLAGS_R         FLAGS_I         FLAGS_Z         FLAGS_Y               RA              DEC COADD_OBJECT_ID   SPREAD_MODEL_G   SPREAD_MODEL_R   SPREAD_MODEL_I   SPREAD_MODEL_Z   SPREAD_MODEL_Y   WAVG_MAG_PSF_G   WAVG_MAG_PSF_R   WAVG_MAG_PSF_I   WAVG_MAG_PSF_Z   WAVG_MAG_PSF_Y WAVG_MAGERR_PSF_G WAVG_MAGERR_PSF_R WAVG_MAGERR_PSF_I WAVG_MAGERR_PSF_Z WAVG_MAGERR_PSF_Y\n",
       "npartitions=10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1152921504606846976  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  int16[pyarrow]  int16[pyarrow]  int16[pyarrow]  int16[pyarrow]  int16[pyarrow]  double[pyarrow]  double[pyarrow]  int64[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]  double[pyarrow]   double[pyarrow]   double[pyarrow]   double[pyarrow]   double[pyarrow]   double[pyarrow]\n",
       "1153202979583557632              ...              ...              ...              ...              ...             ...             ...             ...             ...             ...              ...              ...             ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...               ...               ...               ...               ...               ...\n",
       "...                              ...              ...              ...              ...              ...             ...             ...             ...             ...             ...              ...              ...             ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...               ...               ...               ...               ...               ...\n",
       "2498934843237203968              ...              ...              ...              ...              ...             ...             ...             ...             ...             ...              ...              ...             ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...               ...               ...               ...               ...               ...\n",
       "2499216318213914624              ...              ...              ...              ...              ...             ...             ...             ...             ...             ...              ...              ...             ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...               ...               ...               ...               ...               ...\n",
       "Dask Name: nestedframe, 3 expressions\n",
       "Expr=MapPartitions(NestedFrame)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_deg_cs_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03755b98-beb4-4b12-8934-9c5bc4ca2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(hpms)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086fdf5e-e232-4edd-8436-a9717ba25e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "unrealized = hpms.execute_pipeline(two_deg_cs_catalog, query_string, xmatch_max_neighbors, \n",
    "                                   max_neighbor_dist, min_neighbors, k, max_obj_deviation, des_id_col, mag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f12172-632a-4ee3-a990-bf0bff80b9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:02:55,276 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.04 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:02:55,982 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 6.87 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:02:56,141 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42847\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45086 remote=tcp://127.0.0.1:42847>: Stream is closed\n",
      "2025-06-24 15:02:57,302 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.73 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:02:59,475 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33933\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/utils.py\", line 1910, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 547, in connect\n",
      "    stream = await self.client.connect(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/tornado/tcpclient.py\", line 279, in connect\n",
      "    af, addr, stream = await connector.start(connect_timeout=timeout)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1476, in connect\n",
      "    async with asyncio.timeout(math.inf) as scope:\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/asyncio/timeouts.py\", line 115, in __aexit__\n",
      "    raise TimeoutError from exc_val\n",
      "TimeoutError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1497, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-06-24 15:03:15,545 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 6.48 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:03:15,729 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36279\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39606 remote=tcp://127.0.0.1:36279>: Stream is closed\n",
      "2025-06-24 15:03:33,943 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45527\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44522 remote=tcp://127.0.0.1:45527>: Stream is closed\n",
      "2025-06-24 15:03:54,968 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:03:55,374 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:36225 -> tcp://127.0.0.1:42711\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 1797, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36225 remote=tcp://127.0.0.1:35892>: Stream is closed\n",
      "2025-06-24 15:03:56,235 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42711\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53450 remote=tcp://127.0.0.1:42711>: Stream is closed\n",
      "2025-06-24 15:04:01,206 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:04:01,456 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.66 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:04:01,659 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36041\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/tornado/iostream.py\", line 1113, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 143, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42930 remote=tcp://127.0.0.1:36041>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-06-24 15:04:06,041 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.47 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:04:06,637 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42397\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/tornado/iostream.py\", line 1113, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 143, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35856 remote=tcp://127.0.0.1:42397>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-06-24 15:04:09,820 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 6.89 GiB -- Worker memory limit: 7.42 GiB\n",
      "2025-06-24 15:04:10,131 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42713\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 226, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 237, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 137, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44060 remote=tcp://127.0.0.1:42713>: Stream is closed\n",
      "2025-06-24 15:04:14,147 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('delayed-container-61000c959445ad7e7deb0a2883c27a18', 1))\" coro=<Worker.execute() done, defined at /ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError\n",
      "2025-06-24 15:04:14,147 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('nestedframe-fused-lambda-e42f4991e768fca195003d20266a3cd8', 2))\" coro=<Worker.execute() done, defined at /ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError\n",
      "2025-06-24 15:04:14,154 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('nestedframe-fused-lambda-e42f4991e768fca195003d20266a3cd8', 9))\" coro=<Worker.execute() done, defined at /ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError\n",
      "2025-06-24 15:04:14,155 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('nestedframe-fused-lambda-e42f4991e768fca195003d20266a3cd8', 5))\" coro=<Worker.execute() done, defined at /ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('nestedframe-fused-lambda-e42f4991e768fca195003d20266a3cd8', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41657. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKilledWorker\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Client(n_workers=\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df_with_postfilters = \u001b[43munrealized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_with_postfilters))\n\u001b[32m      5\u001b[39m df_with_postfilters\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/lsdb/catalog/dataset/dataset.py:47\u001b[39m, in \u001b[36mDataset.compute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> npd.NestedFrame:\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute dask distributed dataframe to pandas dataframe\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ddf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/lsdb/nested/core.py:437\u001b[39m, in \u001b[36mNestedFrame.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this Dask collection, returning the underlying dataframe or series.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m npd.NestedFrame(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:491\u001b[39m, in \u001b[36mFrameBase.compute\u001b[39m\u001b[34m(self, fuse, concatenate, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m     out = out.repartition(npartitions=\u001b[32m1\u001b[39m)\n\u001b[32m    490\u001b[39m out = out.optimize(fuse=fuse)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/dask/base.py:370\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    348\u001b[39m \n\u001b[32m    349\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/dask/base.py:656\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    653\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/phy210048p/jpassos/conda-venvs/lsdb-main/lib/python3.12/site-packages/distributed/client.py:2426\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2424\u001b[39m     exception = st.exception\n\u001b[32m   2425\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2428\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mKilledWorker\u001b[39m: Attempted to run task ('nestedframe-fused-lambda-e42f4991e768fca195003d20266a3cd8', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41657. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "with Client(n_workers=5):\n",
    "    df_with_postfilters = unrealized.compute()\n",
    "\n",
    "print(len(df_with_postfilters))\n",
    "df_with_postfilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105078b-a5b7-49b5-a926-00328bc80088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdb-main",
   "language": "python",
   "name": "lsdb-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
